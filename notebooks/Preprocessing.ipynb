{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Import Datasets\n",
    "df_raw = pd.read_csv('/Users/samlai/Documents/Vehicle_insurance_claim_project/data/raw/fraud_oracle.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop useless features, change data type and impute strange values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID Column\n",
    "df_cleaned = df_raw.drop(columns = ['PolicyNumber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RepNumber should be categorical\n",
    "# WeekOfMonth and WeekOfMonthClaimed should be categorical\n",
    "# Deductible should be categorical\n",
    "# Year should be categorical\n",
    "# DriverRating should be categorical\n",
    "df_cleaned['RepNumber'] = df_cleaned['RepNumber'].astype('category')\n",
    "df_cleaned['WeekOfMonth'] = df_cleaned['WeekOfMonth'].astype('category')\n",
    "df_cleaned['WeekOfMonthClaimed'] = df_cleaned['WeekOfMonthClaimed'].astype('category')\n",
    "df_cleaned['Deductible'] = df_cleaned['Deductible'].astype('category')\n",
    "df_cleaned['Year'] = df_cleaned['Year'].astype('category')\n",
    "df_cleaned['DriverRating'] = df_cleaned['DriverRating'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MonthClaimed\n",
       "Jan    1446\n",
       "May    1411\n",
       "Mar    1348\n",
       "Oct    1339\n",
       "Jun    1293\n",
       "Feb    1287\n",
       "Nov    1285\n",
       "Apr    1271\n",
       "Sep    1242\n",
       "Jul    1226\n",
       "Dec    1146\n",
       "Aug    1126\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MonthClaimed中有一個樣本為0\n",
    "# 由於大部分的MonthClaimed與Month的值相同，因此替換成Month裡的'Jul'\n",
    "df_cleaned.loc[df_cleaned['MonthClaimed'] == '0', 'MonthClaimed'] = 'Jul'\n",
    "df_cleaned['MonthClaimed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Binary Variables (AccidentArea, Sex, Fault, PoliceReportFiled, WitnessPresent, AgentType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mapping for AccidentArea:\n",
      "Rural -> 0\n",
      "Urban -> 1\n",
      "\n",
      "Mapping for Sex:\n",
      "Female -> 0\n",
      "Male -> 1\n",
      "\n",
      "Mapping for Fault:\n",
      "Policy Holder -> 0\n",
      "Third Party -> 1\n",
      "\n",
      "Mapping for PoliceReportFiled:\n",
      "No -> 0\n",
      "Yes -> 1\n",
      "\n",
      "Mapping for WitnessPresent:\n",
      "No -> 0\n",
      "Yes -> 1\n",
      "\n",
      "Mapping for AgentType:\n",
      "External -> 0\n",
      "Internal -> 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "binary_columns = [col for col in df_cleaned.columns if len(df_cleaned[col].unique()) == 2 and\n",
    "                  col != 'FraudFound_P']\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in binary_columns:\n",
    "    # Store original values before encoding\n",
    "    original_values = df_cleaned[col].unique()\n",
    "    df_cleaned[col] = le.fit_transform(df_cleaned[col])\n",
    "    # Print mapping for each binary column\n",
    "    print(f\"\\nMapping for {col}:\")\n",
    "    for i, label in enumerate(le.classes_):\n",
    "        print(f\"{label} -> {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Ordinal Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values in DriverRating: [1, 4, 3, 2]\n",
      "Categories (4, int64): [1, 2, 3, 4]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Unique Values in PastNumberOfClaims: ['none' '1' '2 to 4' 'more than 4']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Unique Values in AgeOfVehicle: ['3 years' '6 years' '7 years' 'more than 7' '5 years' 'new' '4 years'\n",
      " '2 years']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Unique Values in NumberOfSuppliments: ['none' 'more than 5' '3 to 5' '1 to 2']\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for col in ['DriverRating', 'PastNumberOfClaims', 'AgeOfVehicle', 'NumberOfSuppliments']:\n",
    "    print(f'Unique Values in {col}: {df_cleaned[col].unique()}')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DriverRating is already numerical, don't need to encode\n",
    "PastNumberClaims_label = {'none': 0, '1': 1, '2 to 4': 2, 'more than 4': 3}\n",
    "AgeOfVehicle_label = {'new': 0, '2 years': 1, '3 years': 2, '4 years': 3, '5 years': 4,\n",
    "                      '6 years': 5, '7 years': 6, 'more than 7': 7}\n",
    "NumberOfSuppliments_label = {'none': 0, '1 to 2': 1, '3 to 5': 2, 'more than 5': 3}\n",
    "\n",
    "df_cleaned['PastNumberOfClaims'] = df_cleaned['PastNumberOfClaims'].map(PastNumberClaims_label)\n",
    "df_cleaned['AgeOfVehicle'] = df_cleaned['AgeOfVehicle'].map(AgeOfVehicle_label)\n",
    "df_cleaned['NumberOfSuppliments'] = df_cleaned['NumberOfSuppliments'].map(NumberOfSuppliments_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Nominal Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA中發現VehiclePrice, Deductible, Age of Policy Holder, Number of Cars, AddressChangeClaim與與詐欺率之間不是簡單的線性關係，因此用One-Hot encoding較適合（可以獨立學習每個類別）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15420, 138)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode Nominal Variables\n",
    "df = pd.get_dummies(df_cleaned, drop_first = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將特徵分組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保險人特徵\n",
    "policyholder_feature = [\n",
    "    'Sex', 'Age', 'DriverRating_2', 'DriverRating_3', 'DriverRating_4',\n",
    "    'AgeOfPolicyHolder_18 to 20', 'AgeOfPolicyHolder_21 to 25',\n",
    "    'AgeOfPolicyHolder_26 to 30', 'AgeOfPolicyHolder_31 to 35', \n",
    "    'AgeOfPolicyHolder_36 to 40', 'AgeOfPolicyHolder_41 to 50',\n",
    "    'AgeOfPolicyHolder_51 to 65', 'AgeOfPolicyHolder_over 65',\n",
    "    'AddressChange_Claim_2 to 3 years', 'AddressChange_Claim_4 to 8 years',\n",
    "    'AddressChange_Claim_no change', 'AddressChange_Claim_under 6 months', 'NumberOfCars_2 vehicles',\n",
    "    'NumberOfCars_3 to 4', 'NumberOfCars_5 to 8', 'NumberOfCars_more than 8',\n",
    "    'PastNumberOfClaims', 'MaritalStatus_Married', 'MaritalStatus_Single',\n",
    "    'MaritalStatus_Widow'\n",
    "]\n",
    "\n",
    "# 車禍事件特徵 \n",
    "accident_feature = [\n",
    "    'AccidentArea', 'Fault', 'PoliceReportFiled', 'NumberOfSuppliments',\n",
    "    'Days_Policy_Accident_more than 30', 'Days_Policy_Accident_15 to 30',\n",
    "    'Days_Policy_Accident_8 to 15', 'Days_Policy_Accident_none',\n",
    "    'Days_Policy_Claim_8 to 15', 'Days_Policy_Claim_more than 30',\n",
    "    'Days_Policy_Claim_none', 'BasePolicy_Collision', 'BasePolicy_Liability',\n",
    "    'Deductible_400', 'Deductible_500', 'Deductible_700',\n",
    "    'PolicyType_Sedan - Collision', 'PolicyType_Sedan - Liability',\n",
    "    'PolicyType_Sport - Collision', 'PolicyType_Sport - All Perils',\n",
    "    'PolicyType_Sport - Liability', 'PolicyType_Utility - All Perils',\n",
    "    'PolicyType_Utility - Collision', 'PolicyType_Utility - Liability', 'WitnessPresent'\n",
    "]\n",
    "\n",
    "# 車輛特徵\n",
    "vehicle_feature = [\n",
    "    'AgeOfVehicle', 'VehiclePrice_30000 to 39000',\n",
    "    'VehiclePrice_40000 to 59000', 'VehiclePrice_less than 20000', 'VehiclePrice_60000 to 69000',\n",
    "    'VehiclePrice_more than 69000', 'Make_Chevrolet', 'Make_Ford',\n",
    "    'Make_Honda', 'Make_Mazda', 'Make_Pontiac', 'Make_Toyota', 'Make_VW',\n",
    "    'Make_BMW', 'Make_Dodge', 'Make_Ferrari', 'Make_Jaguar', 'Make_Lexus',\n",
    "    'Make_Mecedes', 'Make_Mercury', 'Make_Nisson', 'Make_Porche', 'Make_Saab',\n",
    "    'Make_Saturn', 'VehicleCategory_Sport', 'VehicleCategory_Utility'\n",
    "]\n",
    "\n",
    "# 時間特徵\n",
    "time_feature = [\n",
    "    'Month_Aug', 'Month_Dec', 'Month_Feb', 'Month_Jan', 'Month_Jul',\n",
    "    'Month_Jun', 'Month_Mar', 'Month_May', 'Month_Nov', 'Month_Oct',\n",
    "    'Month_Sep', 'WeekOfMonth_2', 'WeekOfMonth_3', 'WeekOfMonth_4',\n",
    "    'WeekOfMonth_5', 'DayOfWeek_Monday', 'DayOfWeek_Saturday',\n",
    "    'DayOfWeek_Sunday', 'DayOfWeek_Thursday', 'DayOfWeek_Tuesday',\n",
    "    'DayOfWeek_Wednesday', 'Year_1995', 'Year_1996',\n",
    "    'DayOfWeekClaimed_Friday', 'DayOfWeekClaimed_Monday', 'DayOfWeekClaimed_Saturday',\n",
    "    'DayOfWeekClaimed_Sunday', 'DayOfWeekClaimed_Thursday', 'DayOfWeekClaimed_Tuesday',\n",
    "    'DayOfWeekClaimed_Wednesday', 'MonthClaimed_Aug', 'MonthClaimed_Dec', 'MonthClaimed_Feb', \n",
    "    'MonthClaimed_Jan', 'MonthClaimed_Jul', 'MonthClaimed_Jun', 'MonthClaimed_Mar', \n",
    "    'MonthClaimed_May', 'MonthClaimed_Nov', 'MonthClaimed_Oct', 'MonthClaimed_Sep', 'WeekOfMonthClaimed_2',\n",
    "    'WeekOfMonthClaimed_3', 'WeekOfMonthClaimed_4', 'WeekOfMonthClaimed_5'\n",
    "]\n",
    "\n",
    "# 其他特徵\n",
    "other_feature = [\n",
    "    'AgentType', 'RepNumber_2', 'RepNumber_3', 'RepNumber_4', 'RepNumber_5',\n",
    "    'RepNumber_6', 'RepNumber_7', 'RepNumber_8', 'RepNumber_9',\n",
    "    'RepNumber_10', 'RepNumber_11', 'RepNumber_12', 'RepNumber_13',\n",
    "    'RepNumber_14', 'RepNumber_15', 'RepNumber_16'\n",
    "]\n",
    "\n",
    "Target = ['FraudFound_P']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未分組的特徵:\n",
      "FraudFound_P\n"
     ]
    }
   ],
   "source": [
    "# 檢查是否有特徵未被分組\n",
    "all_grouped_features = policyholder_feature + accident_feature + vehicle_feature + time_feature + other_feature\n",
    "all_features = df.columns.tolist()\n",
    "\n",
    "\n",
    "ungrouped_features = [f for f in all_features if f not in all_grouped_features]\n",
    "if ungrouped_features:\n",
    "    print(\"未分組的特徵:\")\n",
    "    for f in ungrouped_features:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"所有特徵都已被分組\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train test dataset\n",
    "# 後續只用 train dataset 進行交叉驗證\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=df['FraudFound_P']  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_policyholder = df_train[policyholder_feature]\n",
    "df_accident = df_train[accident_feature]\n",
    "df_vehicle = df_train[vehicle_feature]\n",
    "df_time = df_train[time_feature]\n",
    "df_other = df_train[other_feature]\n",
    "df_target = df_train[Target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Function to evaluate created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the accuracy, precision, recall, and F1 score\n",
    "def get_test_metrics(y_test, y_pred, y_prob):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'AUC: {auc:.4f}')\n",
    "\n",
    "    # Calculate and print confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def cv_evaluation(df_policyholder, df_accident, df_vehicle, df_time, df_other, df_target):\n",
    "    # 合併所有特徵\n",
    "    df_new = pd.concat([df_policyholder, df_accident, df_vehicle, df_time, df_other, df_target], axis=1)\n",
    "    X = df_new.drop(columns=['FraudFound_P'])\n",
    "    y = df_new['FraudFound_P']\n",
    "    \n",
    "    # 定義 5 折交叉驗證\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # 儲存每折的分數\n",
    "    cv_scores = []\n",
    "    \n",
    "    # 進行交叉驗證\n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        # 分割數據\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "        \n",
    "        # 下採樣\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_train_rus, y_train_rus = rus.fit_resample(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # 訓練模型\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_rus, y_train_rus)\n",
    "        \n",
    "        # 預測\n",
    "        y_val_pred = model.predict(X_val_fold)\n",
    "        y_val_prob = model.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        # 計算分數\n",
    "        scores = {\n",
    "            'accuracy': accuracy_score(y_val_fold, y_val_pred),\n",
    "            'precision': precision_score(y_val_fold, y_val_pred),\n",
    "            'recall': recall_score(y_val_fold, y_val_pred),\n",
    "            'f1': f1_score(y_val_fold, y_val_pred),\n",
    "            'auc': roc_auc_score(y_val_fold, y_val_prob)\n",
    "        }\n",
    "        cv_scores.append(scores)\n",
    "    \n",
    "    # 計算平均分數\n",
    "    print(\"Average CV scores:\")\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc']:\n",
    "        mean_score = np.mean([s[metric] for s in cv_scores])\n",
    "        print(f\"{metric}: {mean_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 保留 'AgeOfPolicyHolder'，刪除'Age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV scores:\n",
      "accuracy: 0.6548\n",
      "precision: 0.1387\n",
      "recall: 0.9146\n",
      "f1: 0.2408\n",
      "auc: 0.8142\n"
     ]
    }
   ],
   "source": [
    "df_policyholder_dropAge = df_policyholder.drop(columns = 'Age')\n",
    "cv_evaluation(df_policyholder_dropAge, df_accident, df_vehicle, df_time, df_other, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除Age後，成效有些為提升，因此將Age移除\n",
    "df_policyholder = df_policyholder.drop(columns = 'Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.創造高風險車子品牌標籤特徵 (保留新建的高風險特徵，移除所有Brand Make特徵)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/th/klmxbc1d2x58bny86xqxsjqc0000gn/T/ipykernel_82331/1558674046.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_vehicle['HighRisk_brand'] = ((df_vehicle[high_risk_makes].any(axis=1)) |\n"
     ]
    }
   ],
   "source": [
    "# Create HighRisk_brand feature based on specific makes\n",
    "# Mecedes, BMW, Saab, Saturn, BMW, Ford, Mercury 這些為高風險品牌\n",
    "# Accurca 是One-hot Encoding後的基準類別，其他 Make features若皆為 0 就是Accurca\n",
    "high_risk_makes = ['Make_Mecedes', 'Make_Saab', 'Make_Saturn', 'Make_BMW', 'Make_Ford', 'Make_Mercury']\n",
    "\n",
    "# Get all make columns\n",
    "all_makes = ['Make_Chevrolet', 'Make_Ford', 'Make_Honda', 'Make_Mazda', 'Make_Pontiac', \n",
    "            'Make_Toyota', 'Make_VW', 'Make_BMW', 'Make_Dodge', 'Make_Ferrari', \n",
    "            'Make_Jaguar', 'Make_Lexus', 'Make_Mecedes', 'Make_Mercury', 'Make_Nisson',\n",
    "            'Make_Porche', 'Make_Saab', 'Make_Saturn']\n",
    "\n",
    "# Create HighRisk_brand feature\n",
    "df_vehicle['HighRisk_brand'] = ((df_vehicle[high_risk_makes].any(axis=1)) | \n",
    "                               (df_vehicle[all_makes].sum(axis=1) == 0)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV scores:\n",
      "accuracy: 0.6513\n",
      "precision: 0.1370\n",
      "recall: 0.9106\n",
      "f1: 0.2382\n",
      "auc: 0.8225\n"
     ]
    }
   ],
   "source": [
    "cv_evaluation(df_policyholder, df_accident, df_vehicle, df_time, df_other, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV scores:\n",
      "accuracy: 0.6595\n",
      "precision: 0.1389\n",
      "recall: 0.9011\n",
      "f1: 0.2406\n",
      "auc: 0.8213\n"
     ]
    }
   ],
   "source": [
    "# 試著把Make相關features都刪掉，因為有HighRisk_brand了\n",
    "df_vehicle_withoutMake = df_vehicle.drop(columns = all_makes)\n",
    "cv_evaluation(df_policyholder, df_accident, df_vehicle_withoutMake, df_time, df_other, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將18個Brand Make相關feature都刪除後，recall跟F1 score反而有些為提升，表示一個HighRisk_brand就可以提供足夠資訊\n",
    "# 更新df_vehicle，移除 Brand Make 相關欄位\n",
    "df_vehicle = df_vehicle.drop(columns = all_makes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.移除PolicyType 或BasePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV scores:\n",
      "accuracy: 0.6555\n",
      "precision: 0.1378\n",
      "recall: 0.9038\n",
      "f1: 0.2391\n",
      "auc: 0.8191\n"
     ]
    }
   ],
   "source": [
    "# PolicyType Column為Based Policy與Vehicle Category的組合，移除PolictType測試\n",
    "# Get all PolicyType columns\n",
    "policy_type_cols = [col for col in df_accident.columns if col.startswith('PolicyType')]\n",
    "\n",
    "# Remove PolicyType columns from df_accident\n",
    "df_accident_dropPolicyType = df_accident.drop(columns=policy_type_cols)\n",
    "\n",
    "cv_evaluation(df_policyholder, df_accident_dropPolicyType, df_vehicle, df_time, df_other, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV scores:\n",
      "accuracy: 0.6610\n",
      "precision: 0.1379\n",
      "recall: 0.8875\n",
      "f1: 0.2387\n",
      "auc: 0.8207\n"
     ]
    }
   ],
   "source": [
    "# 移除Based Policy與Vehicle Category\n",
    "# Get all Based Policy columns\n",
    "BasePolicy_cols = [col for col in df_accident.columns if col.startswith('BasePolicy')]\n",
    "df_accident_dropBasePolicy = df_accident.drop(columns=BasePolicy_cols)\n",
    "\n",
    "VehicleCategory_col = [col for col in df_vehicle.columns if col.startswith('VehicleCategory')]\n",
    "df_vehicle_dropCategory = df_vehicle.drop(columns=VehicleCategory_col)\n",
    "\n",
    "cv_evaluation(df_policyholder, df_accident_dropBasePolicy, df_vehicle_dropCategory, df_time, df_other, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accident = df_accident.drop(columns=BasePolicy_cols)\n",
    "df_vehicle = df_vehicle.drop(columns=VehicleCategory_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.'Days_Policy_Accident' and 'Days_Policy_Claim' Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部分類別樣本數較少，將其合併\n",
    "# Create Days_Policy_Accident_morethan30 feature\n",
    "days_policy_accident_col = [col for col in df_accident.columns if col.startswith('Days_Policy_Accident')]\n",
    "df_accident = df_accident.drop(columns=days_policy_accident_col)\n",
    "df_accident['Days_Policy_Accident_morethan30'] = (df_raw['Days_Policy_Accident'] == 'more than 30').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Days_Policy_Claim_morethan30 feature\n",
    "days_policy_claim_col = [col for col in df_accident.columns if col.startswith('Days_Policy_Claim')]\n",
    "df_accident = df_accident.drop(columns=days_policy_claim_col)\n",
    "df_accident['Days_Policy_Claim_morethan30'] = (df_raw['Days_Policy_Claim'] == 'more than 30').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV scores:\n",
      "accuracy: 0.6599\n",
      "precision: 0.1365\n",
      "recall: 0.8794\n",
      "f1: 0.2363\n",
      "auc: 0.8185\n"
     ]
    }
   ],
   "source": [
    "cv_evaluation(df_policyholder, df_accident, df_vehicle, df_time, df_other, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV scores:\n",
      "accuracy: 0.6616\n",
      "precision: 0.1370\n",
      "recall: 0.8781\n",
      "f1: 0.2370\n",
      "auc: 0.8175\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Average CV scores:\n",
      "accuracy: 0.6599\n",
      "precision: 0.1366\n",
      "recall: 0.8794\n",
      "f1: 0.2365\n",
      "auc: 0.8180\n"
     ]
    }
   ],
   "source": [
    "# 因為兩個特徵非常相似，試著刪掉其中一個\n",
    "df_accident_dropAccident = df_accident.drop(columns = 'Days_Policy_Accident_morethan30')\n",
    "cv_evaluation(df_policyholder, df_accident_dropAccident, df_vehicle, df_time, df_other, df_target)\n",
    "print('-'*100)\n",
    "df_accident_dropClaim = df_accident.drop(columns = 'Days_Policy_Claim_morethan30')\n",
    "cv_evaluation(df_policyholder, df_accident_dropClaim, df_vehicle, df_time, df_other, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'Days_Policy_Claim_morethan30'\n",
    "df_accident = df_accident.drop(columns = 'Days_Policy_Claim_morethan30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 合併AddressChange_Claim, Number of Cars中的少數別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將AddressChange_Claim中的'under 6 months', '1 year', '2 to 3 years'合併為 'under 3 years'\n",
    "# 但由於只剩 'under 3 years', 'no change', '4 to 8 years'三個選項，\n",
    "# 因此透過'no change', '4 to 8 years'就可以推斷出'under 3 years'欄位的值，\n",
    "# 不需要創造 'AddressChange_Claim_under 3 years'欄位\n",
    "df_policyholder = df_policyholder.drop(columns = ['AddressChange_Claim_2 to 3 years', 'AddressChange_Claim_under 6 months'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_policyholder['More than 1 car'] = (df_raw['NumberOfCars'] != '1 vehicle').astype(int)\n",
    "NumberOfCars_col = [col for col in df_policyholder.columns if col.startswith('NumberOfCars')]\n",
    "df_policyholder = df_policyholder.drop(columns=NumberOfCars_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 創造高風險保險代理人特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AgentType', 'RepNumber_2', 'RepNumber_3', 'RepNumber_4', 'RepNumber_5',\n",
       "       'RepNumber_6', 'RepNumber_7', 'RepNumber_8', 'RepNumber_9',\n",
       "       'RepNumber_10', 'RepNumber_11', 'RepNumber_12', 'RepNumber_13',\n",
       "       'RepNumber_14', 'RepNumber_15', 'RepNumber_16'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_other.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RepNumber中有許多類別，創造HighRisk_RepNumber可以減少dummy variables的數量\n",
    "RepNumber_col = [col for col in df_other.columns if col.startswith('RepNumber')]\n",
    "df_other = df_other.drop(columns=RepNumber_col)\n",
    "\n",
    "HighRisk_mask = [6, 7, 9, 10, 13]\n",
    "df_other['HighRisk_RepNumber'] = df_raw['RepNumber'].isin(HighRisk_mask).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV scores:\n",
      "accuracy: 0.6597\n",
      "precision: 0.1391\n",
      "recall: 0.9024\n",
      "f1: 0.2410\n",
      "auc: 0.8226\n"
     ]
    }
   ],
   "source": [
    "cv_evaluation(df_policyholder, df_accident, df_vehicle, df_time, df_other, df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 創造高風險自付金額特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deductible_col = [col for col in df_accident.columns if col.startswith('Deductible')]\n",
    "df_accident = df_accident.drop(columns=Deductible_col)\n",
    "df_accident['HiskRisk_DeductibleAMT'] = df_raw['Deductible'].isin([300, 500]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DayOfWeekClaimed中Saturday及Sunday的樣本數較少，合併成Weekend\n",
    "# 合併後會有Monday, Tuesday, Wednesday, Thrusday, Friday, Weekend六個feature\n",
    "# 由於其中5個feature已可提供所有資訊，故將Weekend刪除（Sunday與Saturday)\n",
    "df_time = df_time.drop(columns = ['DayOfWeekClaimed_Saturday', 'DayOfWeekClaimed_Sunday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV scores:\n",
      "accuracy: 0.6636\n",
      "precision: 0.1390\n",
      "recall: 0.8903\n",
      "f1: 0.2405\n",
      "auc: 0.8184\n"
     ]
    }
   ],
   "source": [
    "cv_evaluation(df_policyholder, df_accident, df_vehicle, df_time, df_other, df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12336, 87)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed = pd.concat([df_policyholder, df_accident, df_vehicle, df_time, df_other, df_target], axis = 1)\n",
    "data_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3084, 87)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 對test dataset套用相同的處理\n",
    "df_test = df_test.drop(columns = 'Age')\n",
    "\n",
    "df_test['HighRisk_brand'] = ((df_test[high_risk_makes].any(axis=1)) | \n",
    "                               (df_test[all_makes].sum(axis=1) == 0)).astype(int)\n",
    "\n",
    "df_test = df_test.drop(columns = all_makes)\n",
    "\n",
    "df_test = df_test.drop(columns=BasePolicy_cols)\n",
    "df_test = df_test.drop(columns=VehicleCategory_col)\n",
    "\n",
    "df_test = df_test.drop(columns=days_policy_accident_col)\n",
    "df_test['Days_Policy_Accident_morethan30'] = (df_raw['Days_Policy_Accident'] == 'more than 30').astype(int)\n",
    "\n",
    "df_test = df_test.drop(columns=days_policy_claim_col)\n",
    "df_test['Days_Policy_Claim_morethan30'] = (df_raw['Days_Policy_Claim'] == 'more than 30').astype(int)\n",
    "\n",
    "df_test= df_test.drop(columns = 'Days_Policy_Claim_morethan30')\n",
    "\n",
    "df_test = df_test.drop(columns = ['AddressChange_Claim_2 to 3 years', 'AddressChange_Claim_under 6 months'])\n",
    "\n",
    "df_test['More than 1 car'] = (df_raw['NumberOfCars'] != '1 vehicle').astype(int)\n",
    "df_test = df_test.drop(columns=NumberOfCars_col)\n",
    "\n",
    "df_test = df_test.drop(columns=RepNumber_col)\n",
    "df_test['HighRisk_RepNumber'] = df_raw['RepNumber'].isin(HighRisk_mask).astype(int)\n",
    "\n",
    "df_test = df_test.drop(columns=Deductible_col)\n",
    "df_test['HiskRisk_DeductibleAMT'] = df_raw['Deductible'].isin([300, 500]).astype(int)\n",
    "\n",
    "df_test = df_test.drop(columns = ['DayOfWeekClaimed_Saturday', 'DayOfWeekClaimed_Sunday'])\n",
    "\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = pd.concat([data_processed, df_test], axis=0)\n",
    "data_processed.to_csv('../data/processed/data_processed.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Vehicle Insurance)",
   "language": "python",
   "name": "vehicle_insurance_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
